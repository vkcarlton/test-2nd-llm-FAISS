services:
    cloudflared:
        image: cloudflare/cloudflared:latest
        restart: unless-stopped
        command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TOKEN}
        depends_on:
            - backend
    ollama:
        image: ollama/ollama
        container_name: ollama
        restart: unless-stopped
        ports:
            - "11434:11434"
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [gpu]
        entrypoint:
            [
                "/bin/bash",
                "-c",
                "ollama serve & sleep 5 && ollama pull ${OLLAMA_MODEL} && wait",
            ]
        volumes:
            - ollama-storage:/root/.ollama

    backend:
        build: ./backend
        container_name: backend
        environment:
            - OLLAMA_API=http://ollama:11434/api/generate
            - OLLAMA_MODEL=${OLLAMA_MODEL}
        ports: ["5050:5050"]
        volumes:
            - ./backend/data.csv:/app/data.csv
        depends_on:
            - ollama

volumes:
    ollama-storage:
